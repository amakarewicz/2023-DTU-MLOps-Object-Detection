{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision import datasets, transforms\n",
    "import json\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/detr/modeling_detr.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agama\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\models\\detr\\image_processing_detr.py:773: FutureWarning: The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetrObjectDetectionOutput(loss=None, loss_dict=None, logits=tensor([[[-19.1194,  -0.0893, -11.0154,  ...,  -8.7117,  -9.8543,  10.6531],\n",
      "         [-17.3640,  -1.8035, -14.0219,  ...,  -8.2177,  -4.1631,  11.4021],\n",
      "         [-20.0461,  -0.5837, -11.1060,  ...,  -9.2851, -12.1836,  11.1371],\n",
      "         ...,\n",
      "         [-19.7237,  -1.3058, -10.1437,  ..., -10.4849, -12.9238,  11.2945],\n",
      "         [-12.8113,   1.9661,  -5.0197,  ...,  -6.7028,  -7.3127,   6.2644],\n",
      "         [-17.3271,  -0.8608, -13.1640,  ...,  -6.6456,  -3.8822,  10.6797]]],\n",
      "       grad_fn=<ViewBackward0>), pred_boxes=tensor([[[0.4433, 0.5302, 0.8853, 0.9056],\n",
      "         [0.5494, 0.2517, 0.0529, 0.2015],\n",
      "         [0.4998, 0.5360, 0.9956, 0.9024],\n",
      "         [0.5534, 0.1835, 0.0415, 0.0616],\n",
      "         [0.4998, 0.4116, 0.9948, 0.8237],\n",
      "         [0.5957, 0.5033, 0.8091, 0.9647],\n",
      "         [0.5416, 0.2791, 0.0393, 0.2099],\n",
      "         [0.1607, 0.2160, 0.2274, 0.1434],\n",
      "         [0.1738, 0.1822, 0.1956, 0.0785],\n",
      "         [0.4997, 0.3887, 0.9994, 0.7828],\n",
      "         [0.6100, 0.5136, 0.7813, 0.9454],\n",
      "         [0.4998, 0.5484, 0.9999, 0.8796],\n",
      "         [0.7717, 0.4983, 0.4555, 0.8899],\n",
      "         [0.8818, 0.6115, 0.2329, 0.7410],\n",
      "         [0.5729, 0.5916, 0.8473, 0.7972],\n",
      "         [0.1542, 0.1895, 0.2465, 0.1191],\n",
      "         [0.1416, 0.1758, 0.2823, 0.1619],\n",
      "         [0.7558, 0.4970, 0.4818, 0.9796],\n",
      "         [0.7421, 0.5181, 0.5235, 0.9266],\n",
      "         [0.4997, 0.3921, 0.9994, 0.7880],\n",
      "         [0.6063, 0.5336, 0.7868, 0.9010],\n",
      "         [0.7650, 0.4147, 0.4705, 0.7395],\n",
      "         [0.4997, 0.4985, 1.0000, 0.9790],\n",
      "         [0.5650, 0.6075, 0.8686, 0.7601],\n",
      "         [0.6828, 0.2782, 0.3232, 0.2933],\n",
      "         [0.6098, 0.5066, 0.7824, 0.9440],\n",
      "         [0.7731, 0.4961, 0.4492, 0.9718],\n",
      "         [0.2537, 0.4403, 0.4802, 0.6666],\n",
      "         [0.5547, 0.1967, 0.0447, 0.0872],\n",
      "         [0.4996, 0.4033, 0.9987, 0.8069],\n",
      "         [0.4997, 0.4990, 1.0000, 0.9773],\n",
      "         [0.6281, 0.4141, 0.7423, 0.5757],\n",
      "         [0.4999, 0.6087, 0.9986, 0.7607],\n",
      "         [0.8822, 0.5112, 0.2332, 0.8162],\n",
      "         [0.4997, 0.4153, 0.9999, 0.8347],\n",
      "         [0.5430, 0.2625, 0.0440, 0.2193],\n",
      "         [0.7741, 0.5125, 0.4469, 0.9436],\n",
      "         [0.1685, 0.1967, 0.2115, 0.0983],\n",
      "         [0.3901, 0.4589, 0.3464, 0.5897],\n",
      "         [0.7398, 0.5121, 0.5329, 0.9603],\n",
      "         [0.6089, 0.4963, 0.7787, 0.9830],\n",
      "         [0.5095, 0.4965, 0.9523, 0.9820],\n",
      "         [0.5487, 0.3069, 0.0748, 0.3185],\n",
      "         [0.3322, 0.5440, 0.6561, 0.8807],\n",
      "         [0.1550, 0.1908, 0.2106, 0.0976],\n",
      "         [0.2595, 0.4356, 0.4636, 0.6565],\n",
      "         [0.7412, 0.4389, 0.5226, 0.8286],\n",
      "         [0.5476, 0.3786, 0.0675, 0.4700],\n",
      "         [0.4997, 0.4810, 1.0000, 0.9573],\n",
      "         [0.1884, 0.1911, 0.2761, 0.1068],\n",
      "         [0.4097, 0.4953, 0.8105, 0.9753],\n",
      "         [0.7669, 0.4043, 0.4680, 0.7251],\n",
      "         [0.4999, 0.5972, 0.9970, 0.7880],\n",
      "         [0.2933, 0.5559, 0.5596, 0.8529],\n",
      "         [0.1457, 0.3002, 0.2913, 0.3258],\n",
      "         [0.2969, 0.5452, 0.5898, 0.8838],\n",
      "         [0.1509, 0.1904, 0.1784, 0.0790],\n",
      "         [0.5481, 0.2711, 0.0548, 0.2398],\n",
      "         [0.5494, 0.3011, 0.0660, 0.3074],\n",
      "         [0.4998, 0.4947, 0.9996, 0.9846],\n",
      "         [0.3040, 0.1949, 0.5259, 0.1344],\n",
      "         [0.2557, 0.5448, 0.4700, 0.8727],\n",
      "         [0.9344, 0.1359, 0.1282, 0.0868],\n",
      "         [0.1654, 0.1874, 0.2133, 0.0958],\n",
      "         [0.4998, 0.1010, 0.9981, 0.2064],\n",
      "         [0.2878, 0.5460, 0.5706, 0.8811],\n",
      "         [0.4997, 0.3847, 0.9996, 0.7739],\n",
      "         [0.2527, 0.4495, 0.4723, 0.6744],\n",
      "         [0.1727, 0.1848, 0.1953, 0.0815],\n",
      "         [0.4997, 0.4753, 1.0000, 0.7531],\n",
      "         [0.9401, 0.1480, 0.1176, 0.0632],\n",
      "         [0.5073, 0.5145, 0.9768, 0.9409],\n",
      "         [0.4997, 0.4965, 0.9999, 0.9871],\n",
      "         [0.7288, 0.4205, 0.4430, 0.5660],\n",
      "         [0.6028, 0.5872, 0.7923, 0.7956],\n",
      "         [0.7613, 0.4073, 0.4809, 0.7265],\n",
      "         [0.1630, 0.1976, 0.2197, 0.1083],\n",
      "         [0.4999, 0.6061, 0.9997, 0.7669],\n",
      "         [0.3146, 0.3104, 0.5253, 0.3477],\n",
      "         [0.1521, 0.1904, 0.1936, 0.0920],\n",
      "         [0.8777, 0.5451, 0.2415, 0.8726],\n",
      "         [0.5187, 0.3369, 0.1179, 0.3863],\n",
      "         [0.4999, 0.5327, 0.9998, 0.9063],\n",
      "         [0.4997, 0.3842, 0.9983, 0.7752],\n",
      "         [0.1657, 0.1870, 0.2128, 0.0883],\n",
      "         [0.6087, 0.5146, 0.7825, 0.9435],\n",
      "         [0.8833, 0.5573, 0.2292, 0.8481],\n",
      "         [0.6118, 0.4981, 0.7784, 0.9733],\n",
      "         [0.4999, 0.5956, 0.9997, 0.7883],\n",
      "         [0.5483, 0.3160, 0.0626, 0.3387],\n",
      "         [0.5442, 0.3479, 0.0818, 0.4120],\n",
      "         [0.7640, 0.4087, 0.4730, 0.7331],\n",
      "         [0.5987, 0.4591, 0.8033, 0.8302],\n",
      "         [0.5522, 0.2486, 0.0548, 0.1938],\n",
      "         [0.5495, 0.2864, 0.0529, 0.2583],\n",
      "         [0.5558, 0.5067, 0.8791, 0.9647],\n",
      "         [0.1610, 0.1845, 0.1937, 0.0783],\n",
      "         [0.4997, 0.3726, 0.9978, 0.7490],\n",
      "         [0.7701, 0.4089, 0.4609, 0.7185],\n",
      "         [0.5504, 0.2723, 0.0637, 0.2427]]], grad_fn=<SigmoidBackward0>), auxiliary_outputs=None, last_hidden_state=tensor([[[ 6.1598e-02, -5.1461e-01, -4.0323e-01,  ..., -6.8609e-01,\n",
      "           2.4458e+00,  1.1229e-01],\n",
      "         [-7.6286e-01, -4.9343e-01, -1.7153e+00,  ..., -2.6337e-01,\n",
      "           3.2548e+00,  6.1374e-01],\n",
      "         [-4.7675e-01, -6.4030e-01, -7.8257e-01,  ..., -1.2225e+00,\n",
      "           2.6471e+00, -1.6272e-04],\n",
      "         ...,\n",
      "         [-4.5716e-01, -9.5351e-01, -5.3540e-01,  ..., -4.4069e-01,\n",
      "           2.2883e+00, -3.9116e-02],\n",
      "         [-5.3162e-01, -1.8551e+00, -1.4860e+00,  ...,  2.8945e-01,\n",
      "           2.2507e+00,  7.3355e-01],\n",
      "         [-9.1941e-01, -1.9131e-02, -1.5155e+00,  ..., -3.7177e-01,\n",
      "           3.2195e+00,  5.1125e-01]]], grad_fn=<NativeLayerNormBackward0>), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-0.0544, -0.0425, -0.0307,  ...,  0.1406, -0.3174, -0.0056],\n",
      "         [ 0.0010, -0.0460, -0.0012,  ...,  0.5143, -0.5329,  0.3865],\n",
      "         [-0.0269, -0.0325, -0.0122,  ...,  0.4759, -0.6520,  0.0302],\n",
      "         ...,\n",
      "         [ 0.0641, -0.0229,  0.0221,  ..., -0.4355,  0.1633, -0.0904],\n",
      "         [ 0.0668, -0.0405,  0.0257,  ..., -0.3035,  0.0219, -0.0150],\n",
      "         [-0.0075, -0.0113,  0.0073,  ..., -0.4467,  0.0683, -0.1685]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), encoder_hidden_states=None, encoder_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scores': tensor([0.9982, 0.9960, 0.9955, 0.9988, 0.9987], grad_fn=<IndexBackward0>),\n",
       " 'labels': tensor([75, 75, 63, 17, 17]),\n",
       " 'boxes': tensor([[ 4.0163e+01,  7.0812e+01,  1.7555e+02,  1.1798e+02],\n",
       "         [ 3.3324e+02,  7.2550e+01,  3.6833e+02,  1.8766e+02],\n",
       "         [-2.2602e-02,  1.1496e+00,  6.3973e+02,  4.7376e+02],\n",
       "         [ 1.3241e+01,  5.2055e+01,  3.1402e+02,  4.7093e+02],\n",
       "         [ 3.4540e+02,  2.3854e+01,  6.4037e+02,  3.6872e+02]],\n",
       "        grad_fn=<IndexBackward0>)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['info', 'licenses', 'images', 'annotations', 'categories'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('./annotations/instances_val2017.json')\n",
    "instances = json.load(f)\n",
    "instances.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[332.89, 79.57, 38.65, 106.31]\n",
      "75\n",
      "[41.88, 74.2, 133.15, 45.1]\n",
      "75\n",
      "[1.08, 0.0, 638.56, 473.53]\n",
      "63\n",
      "[1.07, 1.18, 638.93, 477.85]\n",
      "65\n",
      "[17.5, 54.38, 301.25, 415.0]\n",
      "17\n",
      "[347.5, 25.63, 292.5, 343.75]\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# getting annotations for exemplary validation file we have\n",
    "for i in range(len(instances['annotations'])):\n",
    "    if instances['annotations'][i]['image_id'] == 39769:\n",
    "        print(instances['annotations'][i]['bbox'])\n",
    "        print(instances['annotations'][i]['category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the labels dictionary needed for training - should be list of Dicts ? (for a whole batch of images)\n",
    "dict = {'class_labels': torch.LongTensor([75,75,63,65,17,17]),\n",
    "'boxes':torch.FloatTensor([[332.89, 79.57, 38.65, 106.31],\n",
    "[41.88, 74.2, 133.15, 45.1],\n",
    "[1.08, 0.0, 638.56, 473.53],\n",
    "[1.07, 1.18, 638.93, 477.85],\n",
    "[17.5, 54.38, 301.25, 415.0],\n",
    "[347.5, 25.63, 292.5, 343.75]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_labels': tensor([75, 75, 63, 65, 17, 17]),\n",
       " 'boxes': tensor([[332.8900,  79.5700,  38.6500, 106.3100],\n",
       "         [ 41.8800,  74.2000, 133.1500,  45.1000],\n",
       "         [  1.0800,   0.0000, 638.5600, 473.5300],\n",
       "         [  1.0700,   1.1800, 638.9300, 477.8500],\n",
       "         [ 17.5000,  54.3800, 301.2500, 415.0000],\n",
       "         [347.5000,  25.6300, 292.5000, 343.7500]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model(image, labels\u001b[39m=\u001b[39;49m[\u001b[39mdict\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\agama\\anaconda3\\envs\\mlops\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\agama\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\models\\detr\\modeling_detr.py:1447\u001b[0m, in \u001b[0;36mDetrForObjectDetection.forward\u001b[1;34m(self, pixel_values, pixel_mask, decoder_attention_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1444\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m   1446\u001b[0m \u001b[39m# First, sent images through DETR base model to obtain encoder + decoder outputs\u001b[39;00m\n\u001b[1;32m-> 1447\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[0;32m   1448\u001b[0m     pixel_values,\n\u001b[0;32m   1449\u001b[0m     pixel_mask\u001b[39m=\u001b[39;49mpixel_mask,\n\u001b[0;32m   1450\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   1451\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[0;32m   1452\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1453\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   1454\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1455\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1456\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1457\u001b[0m )\n\u001b[0;32m   1459\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1461\u001b[0m \u001b[39m# class logits + predicted bounding boxes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\agama\\anaconda3\\envs\\mlops\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\agama\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\models\\detr\\modeling_detr.py:1273\u001b[0m, in \u001b[0;36mDetrModel.forward\u001b[1;34m(self, pixel_values, pixel_mask, decoder_attention_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1268\u001b[0m output_hidden_states \u001b[39m=\u001b[39m (\n\u001b[0;32m   1269\u001b[0m     output_hidden_states \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states\n\u001b[0;32m   1270\u001b[0m )\n\u001b[0;32m   1271\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1273\u001b[0m batch_size, num_channels, height, width \u001b[39m=\u001b[39m pixel_values\u001b[39m.\u001b[39;49mshape\n\u001b[0;32m   1274\u001b[0m device \u001b[39m=\u001b[39m pixel_values\u001b[39m.\u001b[39mdevice\n\u001b[0;32m   1276\u001b[0m \u001b[39mif\u001b[39;00m pixel_mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\agama\\anaconda3\\envs\\mlops\\lib\\site-packages\\PIL\\Image.py:517\u001b[0m, in \u001b[0;36mImage.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    515\u001b[0m     deprecate(\u001b[39m\"\u001b[39m\u001b[39mImage categories\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mis_animated\u001b[39m\u001b[39m\"\u001b[39m, plural\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_category\n\u001b[1;32m--> 517\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: shape"
     ]
    }
   ],
   "source": [
    "model(image, labels=[dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "022e507a94ba81639724ef722e08e60a731055d02a6260ec21d3f19c85df79de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
